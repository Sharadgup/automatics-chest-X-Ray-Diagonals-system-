# -*- coding: utf-8 -*-
"""chest x-ray model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTk91ztT0K8qlTKQ8otQF5U5CCycAyTZ

# This is automatics chest x ray diagonals model

# Import required to this model

# create the Data pipeline of Kaggle API to using Dataset import Server
"""

pip install kaggle

"""# Import Kaggle Json file dataset server enable"""

!echo '{"username":"sharadgupta45","key":"143d00c675ef6ae3a6fd51397a065fe7"}' > kaggle.json

"""# check the os and Shutil path for Json file exited are not"""

import shutil
import os

# Check if the source file exists
source_file = '/root/.kaggle/kaggle (1).json'
if os.path.exists(source_file):
    # Move the file
    shutil.move(source_file, os.path.expanduser('~/.kaggle/kaggle (1).json'))
else:
    print("The source file does not exist.")

"""# Again Kaggle Json file are not EXected"""

import os

# Define the path to the kaggle.json file
kaggle_json_path = '/root/.kaggle/kaggle (1).json'

# Check if the file exists
if os.path.exists(kaggle_json_path):
    print("kaggle.json file exists.")

    # Check the permissions of the directory
    directory_permissions = oct(os.stat('/root/.kaggle').st_mode)[-3:]
    print(f"Permissions of the .kaggle directory: {directory_permissions}")

    # Check the permissions of the file
    file_permissions = oct(os.stat(kaggle_json_path).st_mode)[-3:]
    print(f"Permissions of kaggle.json file: {file_permissions}")
else:
    print("kaggle.json file does not exist.")

"""# using Linux python chmod command find the Kaggle Json file"""

!chmod 600 /root/.kaggle/kaggle (1).json

"""# create Environment for Kaggle Api link in using Data Pipeline and Connect APi Data Channel"""

# Commented out IPython magic to ensure Python compatibility.
# %env KAGGLE_USERNAME=sharadgupta45
# %env KAGGLE_KEY=143d00c675ef6ae3a6fd51397a065fe7

"""# Calling for Data pipeline Kaggle APi for using Extended environment"""

from kaggle.api.kaggle_api_extended import KaggleApi

# Define your Kaggle API key
api_key = "143d00c675ef6ae3a6fd51397a065fe7"

# Set Kaggle API key
api = KaggleApi()
api.api_key = api_key

# Authenticate with Kaggle API
api.authenticate()

# Example: List datasets
datasets = api.dataset_list()
for dataset in datasets:
    print(dataset.ref)

"""# Using Ls Command for Linux in Create are Root Dir to Sub Dir it is Name as .kaggle"""

ls /root/.kaggle/

"""# again check"""

!chmod 600 "/root/.kaggle/kaggle.json"

"""# Calling Data Pipeline Api for Kaggle Dataset it is Name as chest X ray Diagonal"""

from kaggle.api.kaggle_api_extended import KaggleApi

# Instantiate the Kaggle API
api = KaggleApi()

# Authenticate with the Kaggle API using your API key
api.authenticate()

# Download the dataset
api.dataset_download_files('paultimothymooney/chest-xray-pneumonia', path='https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia', unzip=True)

"""# Again this processing apply"""

from kaggle.api.kaggle_api_extended import KaggleApi
import os

# Instantiate the Kaggle API
api = KaggleApi()

# Authenticate with the Kaggle API using your API key
api.authenticate()  # Make sure your kaggle.json file is correctly configured

# Specify the name of the dataset you want to download
dataset_name = 'paultimothymooney/chest-xray-pneumonia'

# Specify the directory where you want to download the dataset
download_dir = 'https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia'

# Debugging statements
print(f"Dataset Name: {dataset_name}")
print(f"Download Directory: {download_dir}")

# Download the dataset
try:
    api.dataset_download_files(dataset_name, path=download_dir, quiet=False, unzip=True)
    print("Dataset downloaded successfully!")
except Exception as e:
    print(f"An error occurred: {e}")

# Check if the dataset file exists in the download directory
dataset_file_path = os.path.join(download_dir, dataset_name + '.zip')
if os.path.exists(dataset_file_path):
    # Dataset file exists, you can now use it as needed
    print("Dataset file found:", dataset_file_path)
    # Now you can further process the dataset file as needed
else:
    print("Dataset file not found:", dataset_file_path)

"""# One Successful to Data pipeline are connect and Easy Access to make a download for Dataset into google Colab Engine"""

!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia

"""# Upzip for File for Download the Dataset Find image will be exected"""

!unzip chest-xray-pneumonia.zip

"""# Import library using Deep learning And Self supervison learning"""

import os
import shutil
import itertools
import pathlib
from PIL import Image

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('whitegrid')
import plotly.express as px

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D , MaxPooling2D , Flatten , Activation , Dense , Dropout , BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras import regularizers

import warnings
warnings.filterwarnings('ignore')

"""# Local Path are Google Colab Disk will be define and Call for Training Dataset"""

train_data_path = '/content/chest_xray/train'
filepaths =[]
labels = []
folds = os.listdir(train_data_path)

for fold in folds:
    f_path = os.path.join(train_data_path , fold)
    filelists = os.listdir(f_path)

    for file in filelists:
        filepaths.append(os.path.join(f_path , file))
        labels.append(fold)

Fseries = pd.Series(filepaths , name = 'filepaths')
Lseries = pd.Series(labels , name = 'label')
df = pd.concat([Fseries , Lseries] , axis = 1)

"""# Convert for Dataset to image into CsV file way dataset should be print"""

df

"""# Data Presentation Using Graph plot"""

# @title label

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""# Local Path are Google Colab Disk will be define and Call for Test dataset"""

test_data_path = '/content/chest_xray/test'

filepaths =[]
labels = []
folds = os.listdir(test_data_path)

for fold in folds:
    f_path = os.path.join(test_data_path , fold)
    filelists = os.listdir(f_path)

    for file in filelists:
        filepaths.append(os.path.join(f_path , file))
        labels.append(fold)

Fseries = pd.Series(filepaths , name = 'filepaths')
Lseries = pd.Series(labels , name = 'label')
test = pd.concat([Fseries , Lseries] , axis = 1)
test

# @title label

from matplotlib import pyplot as plt
import seaborn as sns
test.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""# Local Path are Google Colab Disk will be define and Call for value dataset"""

valid_data_path = '/content/chest_xray/val'

filepaths =[]
labels = []
folds = os.listdir(test_data_path)

for fold in folds:
    f_path = os.path.join(test_data_path , fold)
    filelists = os.listdir(f_path)

    for file in filelists:
        filepaths.append(os.path.join(f_path , file))
        labels.append(fold)

Fseries = pd.Series(filepaths , name = 'filepaths')
Lseries = pd.Series(labels , name = 'label')
valid = pd.concat([Fseries , Lseries] , axis = 1)
valid

# @title label

from matplotlib import pyplot as plt
import seaborn as sns
valid.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

fig= px.histogram(data_frame= df,
           y= df['label'],
           template='plotly_dark',
           color= df['label'].values,
           title='number of images in each class of the train data')
fig.show()

import plotly.express as px

# Data checking and input validation
if 'label' in test.columns and not test.empty:
    # Create the histogram
    fig = px.histogram(
        data_frame=test,
        x='label',  # Changed to x-axis for better readability
        template='plotly_dark',
        title='Number of images in each class of the test data'
    )
    # Show the histogram
    fig.show()
else:
    print("Error: 'label' column not found in the DataFrame or DataFrame is empty.")

fig= px.histogram(data_frame= valid,
           y= valid['label'],
           template='plotly_dark',
           color=valid['label'].values,
           title='number of images in each class of the valid data')
fig.show()

"""# Training Dataset Sample Dummy of Data spilt given to 80 % Train and 20 % Test"""

train_df, dummy_df = train_test_split(df, train_size=0.8, shuffle= True, random_state= 42)
valid_df, test_df= train_test_split(dummy_df, train_size= 0.6, shuffle= True, random_state= 42)

"""# convert csv file image dataset and Image processing and image Scalar for Size and shape"""

img_size = (224 ,224)
batch_size = 16
img_shape= (img_size[0], img_size[1], 3)

def scalar(img):
    return img

tr_gen = ImageDataGenerator(preprocessing_function= scalar)
ts_gen = ImageDataGenerator(preprocessing_function= scalar)

train_gen = tr_gen.flow_from_dataframe(train_df , x_col = 'filepaths' , y_col = 'label' , target_size = img_size ,
                                      class_mode = 'categorical' , color_mode = 'rgb' , shuffle = True , batch_size =batch_size)
valid_gen = ts_gen.flow_from_dataframe(valid_df , x_col = 'filepaths' , y_col = 'label' , target_size = img_size ,
                                       class_mode = 'categorical',color_mode = 'rgb' , shuffle= True, batch_size = batch_size)
test_gen = ts_gen.flow_from_dataframe(test_df , x_col= 'filepaths' , y_col = 'label' , target_size = img_size ,
                                      class_mode = 'categorical' , color_mode= 'rgb' , shuffle = False , batch_size = batch_size)

"""# Image generation dict for Train for Class dataset to define"""

gen_dict = train_gen.class_indices
classes = list(gen_dict.keys())
images , labels = next(train_gen)

plt.figure(figsize= (20,20))

for i in range(16):
    plt.subplot(4,4,i+1)
    image = images[i] / 255
    plt.imshow(image)
    index = np.argmax(labels[i])
    class_name = classes[index]
    plt.title(class_name , color = 'blue' , fontsize= 12)
    plt.axis('off')
plt.show();

"""# Train Dataset to Tersonflow library to Keras application to model train catagrorical for image"""

img_size = (224, 224)
img_shape = (img_size[0] , img_size[1] , 3)
num_class = len(classes)
base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top = False , weights = 'imagenet',input_shape = img_shape, pooling= 'max')
model = Sequential([
    base_model,
    BatchNormalization(axis= -1 , momentum= 0.99 , epsilon= 0.001),
    Dense(256, kernel_regularizer = regularizers.l2(l= 0.016) , activity_regularizer = regularizers.l1(0.006),bias_regularizer= regularizers.l1(0.006) , activation = 'relu'),
    Dropout(rate= 0.4 , seed = 75),
    Dense(num_class , activation = 'softmax')
])
model.compile(Adamax(learning_rate = 0.0001) , loss = 'categorical_crossentropy', metrics = ['accuracy'])
model.summary()

"""# Model Training and find to fit In this model was Mulit Dimensional or Mulit Layer train like concept for LLM (Large Learning Model )"""

Epochs = 2
history = model.fit(x= train_gen , epochs = Epochs, verbose = 1, validation_data= valid_gen,validation_steps = None , shuffle = False)

"""# Evaluation"""

train_acc = history.history['accuracy']
train_loss = history.history['loss']

val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']

index_loss = np.argmin(val_loss)
val_lowest = val_loss[index_loss]

index_acc = np.argmax(val_acc)
val_highest = val_acc[index_acc]

Epochs = [i+1 for i in range(len(train_acc))]

loss_label = f'Best epochs = {str(index_loss +1)}'
acc_label = f'Best epochs = {str(index_acc + 1)}'

#Training history

plt.figure(figsize= (20,8))
plt.style.use('fivethirtyeight')

plt.subplot(1,2,1)
plt.plot(Epochs , train_loss , 'r' , label = 'Training Loss')
plt.plot(Epochs , val_loss , 'g' , label = 'Validation Loss')
plt.scatter(index_loss + 1 , val_lowest , s = 150 , c = 'blue',label = loss_label)
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(Epochs , train_acc , 'r' , label = 'Training Accuracy')
plt.plot(Epochs , val_acc , 'g' , label = 'Validation Accuracy')
plt.scatter(index_acc + 1 , val_highest , s = 150 , c = 'blue',label = acc_label)
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.tight_layout
plt.show();

train_score = model.evaluate(train_gen , steps =16 , verbose = 1)
valid_score = model.evaluate(valid_gen , steps = 16 , verbose = 1)
test_score = model.evaluate(test_gen , steps = 16 , verbose = 1)

print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Validation Loss: ", valid_score[0])
print("Validation Accuracy: ", valid_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

"""# Predictions"""

preds = model.predict_generator(test_gen)
y_pred = np.argmax(preds , axis = 1)

"""# Confusion matrix and Classification report"""

g_dict = test_gen.class_indices
classes = list(g_dict.keys())
cm = confusion_matrix(test_gen.classes, y_pred)

plt.figure(figsize= (10, 10))
plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation= 45)
plt.yticks(tick_marks, classes)


thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')

plt.tight_layout()
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

plt.show()

"""# Print to classification report model test gen"""

print(classification_report(test_gen.classes, y_pred , target_names= classes ))